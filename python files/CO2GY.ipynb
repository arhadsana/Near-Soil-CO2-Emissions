{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "t7zlQVsQILKh"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s03dfWib-K3j"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/Colab Notebooks"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error"
      ],
      "metadata": {
        "id": "x9T_uLdv-mP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "df = pd.read_csv('CO2n.csv', nrows=10000, encoding='utf-8-sig')\n",
        "\n",
        "# Extract target variables\n",
        "y = df[['CO2']]\n",
        "X = df.drop(['Date', 'AP', 'RAD' ,'CO2'], axis=1)"
      ],
      "metadata": {
        "id": "SKS1u5JK-trK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.head()"
      ],
      "metadata": {
        "id": "W2NYsjvb-wQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train, validation, and test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n"
      ],
      "metadata": {
        "id": "3MOOtSoC-3i1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "#  Perform RFE\n",
        "# Initialize the RFE selector with a Linear Regression model and specify the number of features to select\n",
        "rfe_selector = RFE(estimator=LinearRegression(), n_features_to_select=5)\n",
        "\n",
        "# Fit the RFE selector to the training data\n",
        "rfe_selector.fit(X_train, y_train)\n",
        "\n",
        "# Get the selected features\n",
        "selected_features = X_train.columns[rfe_selector.support_]\n",
        "\n",
        "print(\"Selected features by RFE:\")\n",
        "print(selected_features)"
      ],
      "metadata": {
        "id": "nzNon_8idyDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLGRIDSEARCH"
      ],
      "metadata": {
        "id": "ZVzfr1lm_m9W"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b328b445-c05d-4753-8e55-64b8f9752e2d"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "\n",
        "# Step 1: Define Models and Parameter Grids\n",
        "\n",
        "# Linear Regression (no hyperparameters to tune with GridSearchCV)\n",
        "#lr = MultiOutputRegressor(LinearRegression())\n",
        "#lr_param_grid = {}\n",
        "\n",
        "# KNN Regressor\n",
        "knn = MultiOutputRegressor(KNeighborsRegressor())\n",
        "knn_param_grid = {'estimator__n_neighbors': [3, 5, 7, 9]}\n",
        "\n",
        "# Random Forest Regressor\n",
        "rf = MultiOutputRegressor(RandomForestRegressor(random_state=42))\n",
        "rf_param_grid = {'estimator__n_estimators': [50, 100, 150],\n",
        "                 'estimator__max_depth': [None, 10, 20]}\n",
        "\n",
        "# Gradient Boosting Regressor\n",
        "gbr = MultiOutputRegressor(GradientBoostingRegressor(random_state=42))\n",
        "gbr_param_grid = {'estimator__n_estimators': [50, 100, 150],\n",
        "                  'estimator__learning_rate': [0.01, 0.1,],\n",
        "                  'estimator__max_depth': [3, 5, 7]}\n",
        "\n",
        "\n",
        "# XGBoost Regressor\n",
        "xgb = MultiOutputRegressor(XGBRegressor(random_state=42))\n",
        "xgb_param_grid = {'estimator__n_estimators': [50, 100, 150],\n",
        "                  'estimator__learning_rate': [0.01, 0.1,],\n",
        "                  'estimator__max_depth': [3, 5,]}\n",
        "\n",
        "# Ridge Regressor\n",
        "ridge = MultiOutputRegressor(Ridge(random_state=42))\n",
        "ridge_param_grid = {'estimator__alpha': [0.1, 1.0, 10.0]}\n",
        "\n",
        "# LASSO Regressor\n",
        "lasso = MultiOutputRegressor(Lasso(random_state=42))\n",
        "lasso_param_grid = {'estimator__alpha': [0.1, 1.0, 10.0]}\n",
        "\n",
        "# SVR (Support Vector Regressor) - Note: SVR can be computationally expensive\n",
        "svr = MultiOutputRegressor(SVR())\n",
        "svr_param_grid = {'estimator__C': [0.1, 1.0, 10.0],\n",
        "                  'estimator__epsilon': [0.01, 0.1, 0.2]}\n",
        "\n",
        "# MLP Regressor (Neural Network)\n",
        "mlp = MultiOutputRegressor(MLPRegressor(random_state=42))\n",
        "mlp_param_grid = {'estimator__hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],\n",
        "                  'estimator__activation': ['relu', 'tanh', 'logistic'],\n",
        "                  'estimator__solver': ['adam', 'sgd'],\n",
        "                  'estimator__learning_rate_init': [0.001, 0.01, 0.1],\n",
        "                  'estimator__max_iter': [1000, 2000, 3000]}\n",
        "\n",
        "\n",
        "models_grid = [\n",
        "    #(lr, lr_param_grid, 'Linear Regression'),\n",
        "    (knn, knn_param_grid, 'KNN'),\n",
        "    (rf, rf_param_grid, 'Random Forest'),\n",
        "    (gbr, gbr_param_grid, 'Gradient Boosting'),\n",
        "    (xgb, xgb_param_grid, 'XGBoost'),\n",
        "    (ridge, ridge_param_grid, 'Ridge'),\n",
        "    (lasso, lasso_param_grid, 'LASSO'),\n",
        "    (svr, svr_param_grid, 'SVR'),\n",
        "    (mlp, mlp_param_grid, 'MLP'),\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be8671cc"
      },
      "source": [
        "# Step 2: Implement Grid Search with Cross-Validation and Capture All Metrics\n",
        "\n",
        "optimized_models = {}\n",
        "results_list = []\n",
        "\n",
        "for model, param_grid, name in models_grid:\n",
        "    print(f\"Running GridSearchCV for {name}...\")\n",
        "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    best_model = grid_search.best_estimator_\n",
        "    optimized_models[name] = best_model\n",
        "\n",
        "    print(f\"Best parameters for {name}: {grid_search.best_params_}\")\n",
        "\n",
        "    # Evaluate the best model on the train, validation, and test sets\n",
        "    y_train_pred = best_model.predict(X_train)\n",
        "    y_val_pred = best_model.predict(X_val)\n",
        "    y_test_pred = best_model.predict(X_test)\n",
        "\n",
        "\n",
        "    # Calculate metrics for each output on train, validation, and test sets\n",
        "    model_results = {'Model': name}\n",
        "    for i, col in enumerate(y_test.columns): # Assuming the same columns for train, val, test\n",
        "        model_results[f'{col}_Train_RMSE'] = np.sqrt(mean_squared_error(y_train[col], y_train_pred[:, i]))\n",
        "        model_results[f'{col}_Train_R2'] = r2_score(y_train[col], y_train_pred[:, i])\n",
        "        model_results[f'{col}_Train_MAE'] = mean_absolute_error(y_train[col], y_train_pred[:, i])\n",
        "        model_results[f'{col}_Train_MAPE'] = mean_absolute_percentage_error(y_train[col], y_train_pred[:, i])\n",
        "\n",
        "        model_results[f'{col}_Val_RMSE'] = np.sqrt(mean_squared_error(y_val[col], y_val_pred[:, i]))\n",
        "        model_results[f'{col}_Val_R2'] = r2_score(y_val[col], y_val_pred[:, i])\n",
        "        model_results[f'{col}_Val_MAE'] = mean_absolute_error(y_val[col], y_val_pred[:, i])\n",
        "        model_results[f'{col}_Val_MAPE'] = mean_absolute_percentage_error(y_val[col], y_val_pred[:, i])\n",
        "\n",
        "\n",
        "        model_results[f'{col}_Test_RMSE'] = np.sqrt(mean_squared_error(y_test[col], y_test_pred[:, i]))\n",
        "        model_results[f'{col}_Test_R2'] = r2_score(y_test[col], y_test_pred[:, i])\n",
        "        model_results[f'{col}_Test_MAE'] = mean_absolute_error(y_test[col], y_test_pred[:, i])\n",
        "        model_results[f'{col}_Test_MAPE'] = mean_absolute_percentage_error(y_test[col], y_test_pred[:, i])\n",
        "\n",
        "\n",
        "    results_list.append(model_results)\n",
        "\n",
        "print(\"Grid search optimization and evaluation completed.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e163b0db"
      },
      "source": [
        "# Step 3: Compare Model Performance and Export All Metrics\n",
        "\n",
        "results_df_optimized_all_metrics = pd.DataFrame(results_list)\n",
        "print(\"Optimized Model Performance (Train, Validation, and Test Sets):\")\n",
        "print(results_df_optimized_all_metrics)\n",
        "\n",
        "# Export all evaluation metrics to CSV in the CO2 folder\n",
        "results_df_optimized_all_metrics.to_csv('CO2/optimized_model_performance_all_metrics.csv', index=False)\n",
        "print(\"Optimized model performance (all metrics) exported to CO2/optimized_model_performance_all_metrics.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c00127d4"
      },
      "source": [
        "# Step 4: Generate Predictions for All Models\n",
        "\n",
        "all_predictions = {}\n",
        "\n",
        "for name, model in optimized_models.items():\n",
        "    print(f\"Generating predictions for {name}...\")\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_val_pred = model.predict(X_val)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "\n",
        "    all_predictions[name] = {\n",
        "        'train': y_train_pred,\n",
        "        'val': y_val_pred,\n",
        "        'test': y_test_pred\n",
        "    }\n",
        "\n",
        "print(\"Prediction generation completed.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "465dfd2e"
      },
      "source": [
        "# Step 5: Retrieve Dates for Each Set\n",
        "\n",
        "train_dates = df.loc[X_train.index, 'Date']\n",
        "val_dates = df.loc[X_val.index, 'Date']\n",
        "test_dates = df.loc[X_test.index, 'Date']\n",
        "\n",
        "print(\"Dates retrieved for train, validation, and test sets.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8c300195"
      },
      "source": [
        "# Step 6: Prepare and Export DataFrames\n",
        "\n",
        "for model_name, predictions in all_predictions.items():\n",
        "    print(f\"Exporting predictions for {model_name}...\")\n",
        "\n",
        "    # Train set\n",
        "    train_df_export = pd.DataFrame({\n",
        "        'Date': train_dates,\n",
        "        'Actual_CO2': y_train['CO2'],\n",
        "        'Predicted_CO2': predictions['train'].flatten() # Flatten in case of multi-output but single target\n",
        "    })\n",
        "    train_df_export.to_csv(f'CO2/{model_name.replace(\" \", \"_\")}_train_predictions.csv', index=False)\n",
        "\n",
        "    # Validation set\n",
        "    val_df_export = pd.DataFrame({\n",
        "        'Date': val_dates,\n",
        "        'Actual_CO2': y_val['CO2'],\n",
        "        'Predicted_CO2': predictions['val'].flatten()\n",
        "    })\n",
        "    val_df_export.to_csv(f'CO2/{model_name.replace(\" \", \"_\")}_val_predictions.csv', index=False)\n",
        "\n",
        "    # Test set\n",
        "    test_df_export = pd.DataFrame({\n",
        "        'Date': test_dates,\n",
        "        'Actual_CO2': y_test['CO2'],\n",
        "        'Predicted_CO2': predictions['test'].flatten()\n",
        "    })\n",
        "    test_df_export.to_csv(f'CO2/{model_name.replace(\" \", \"_\")}_test_predictions.csv', index=False)\n",
        "\n",
        "print(\"Prediction export completed.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PART2**"
      ],
      "metadata": {
        "id": "f-Um-vGh9Wfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Select Base Models\n",
        "\n",
        "# Sort the results by Test R2 in descending order\n",
        "sorted_models = results_df_optimized_all_metrics.sort_values(by='CO2_Test_R2', ascending=False)\n",
        "\n",
        "# Select the top 3 models\n",
        "selected_base_model_names = sorted_models['Model'].head(3).tolist()\n",
        "\n",
        "\n",
        "print(\"Top 3 base models selected for stacking:\")\n",
        "print(selected_base_model_names)\n",
        "\n",
        "# Retrieve the actual model objects from the optimized_models dictionary\n",
        "selected_base_models = [(name, optimized_models[name]) for name in selected_base_model_names]"
      ],
      "metadata": {
        "id": "qBrOsC4ZJYK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Prepare Data for Stacking\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Define the number of folds\n",
        "n_splits = 5\n",
        "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "print(f\"Training data split into {n_splits} folds for stacking.\")"
      ],
      "metadata": {
        "id": "fLsrwDcnMcHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fb4da26"
      },
      "source": [
        "# Step 3: Train Base Models and Generate Out-of-Fold Predictions\n",
        "\n",
        "oof_train_preds = np.zeros((X_train.shape[0], len(selected_base_models)))\n",
        "\n",
        "for i, (name, model) in enumerate(selected_base_models):\n",
        "    print(f\"Generating out-of-fold predictions for {name}...\")\n",
        "    oof_preds = np.zeros((X_train.shape[0],))\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train, y_train)):\n",
        "        X_train_fold, y_train_fold = X_train.iloc[train_idx], y_train.iloc[train_idx]\n",
        "        X_val_fold, y_val_fold = X_train.iloc[val_idx], y_train.iloc[val_idx]\n",
        "\n",
        "        model.fit(X_train_fold, y_train_fold)\n",
        "        oof_preds[val_idx] = model.predict(X_val_fold).flatten() # Flatten in case of multi-output but single target\n",
        "\n",
        "    oof_train_preds[:, i] = oof_preds\n",
        "\n",
        "print(\"Out-of-fold prediction generation completed.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aa84ed96"
      },
      "source": [
        "# Step 4: Generate Test Predictions from Base Models\n",
        "\n",
        "test_preds_base_models = np.zeros((X_test.shape[0], len(selected_base_models)))\n",
        "\n",
        "for i, (name, model) in enumerate(selected_base_models):\n",
        "    print(f\"Generating test predictions for {name}...\")\n",
        "    # Train the base model on the full training data\n",
        "    model.fit(X_train, y_train)\n",
        "    # Generate predictions on the test set\n",
        "    test_preds_base_models[:, i] = model.predict(X_test).flatten() # Flatten in case of multi-output but single target\n",
        "\n",
        "print(\"Test prediction generation from base models completed.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5 (Optimized): Train Ridge as Meta-Learner with GridSearchCV\n",
        "\n",
        "# Define Ridge Regressor as the meta-learner\n",
        "ridge_meta_learner = Ridge(random_state=42)\n",
        "\n",
        "# Define the parameter grid for Ridge\n",
        "ridge_param_grid_meta = {\n",
        "    'alpha': [0.1, 1.0, 10.0, 100.0] # Experiment with different alpha values\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search_meta = GridSearchCV(ridge_meta_learner, ridge_param_grid_meta, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "\n",
        "# Fit GridSearchCV to the out-of-fold predictions\n",
        "grid_search_meta.fit(oof_train_preds, y_train)\n",
        "\n",
        "# Get the best meta-learner model\n",
        "meta_learner_ridge_optimized = grid_search_meta.best_estimator_\n",
        "\n",
        "print(\"Optimized Ridge meta-learner training completed.\")\n",
        "print(\"Best parameters for Ridge meta-learner:\", grid_search_meta.best_params_)\n",
        "# Step 6: Generate Final Predictions\n",
        "\n",
        "stacked_test_preds = meta_learner_ridge_optimized.predict(test_preds_base_models)\n",
        "\n",
        "print(\"Final stacked model predictions generated.\")\n"
      ],
      "metadata": {
        "id": "VMCqgdJDnbNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc3d8da0"
      },
      "source": [
        "# Step 7: Evaluate Stacked Model\n",
        "\n",
        "stacked_test_rmse = np.sqrt(mean_squared_error(y_test, stacked_test_preds))\n",
        "stacked_test_r2 = r2_score(y_test, stacked_test_preds)\n",
        "stacked_test_mae = mean_absolute_error(y_test, stacked_test_preds)\n",
        "stacked_test_mape = mean_absolute_percentage_error(y_test, stacked_test_preds)\n",
        "\n",
        "print(\"Stacked Model Test RMSE:\", stacked_test_rmse)\n",
        "print(\"Stacked Model Test R2:\", stacked_test_r2)\n",
        "print(\"Stacked Model Test MAE:\", stacked_test_mae)\n",
        "print(\"Stacked Model Test MAPE:\", stacked_test_mape)\n",
        "\n",
        "# Step 8: Compare Stacked Model Performance\n",
        "\n",
        "#stacked_results = {\n",
        " #   'Model': 'Stacked Model (Ridge Meta-Learner)',\n",
        "  #  'CO2_Train_RMSE': np.nan, # Not directly calculated in this stacking approach\n",
        "   # 'CO2_Train_R2': np.nan,   # Not directly calculated\n",
        "  #  'CO2_Train_MAE': np.nan,  # Not directly calculated\n",
        "   # 'CO2_Train_MAPE': np.nan, # Not directly calculated\n",
        "    #'CO2_Val_RMSE': np.nan,   # Not directly calculated\n",
        "    #'#CO2_Val_R2': np.nan,     # Not directly calculated\n",
        "    #'CO2_Val_MAE': np.nan,    # Not directly calculated\n",
        "    #'CO2_Val_MAPE': np.nan,   # Not directly calculated\n",
        "    #'CO2_Test_RMSE': stacked_test_rmse,\n",
        "    #'CO2_Test_R2': stacked_test_r2,\n",
        "    #'CO2_Test_MAE': stacked_test_mae,\n",
        "    #'CO2_Test_MAPE': stacked_test_mape\n",
        "#}\n",
        "\n",
        "#results_df_optimized_all_metrics = pd.concat([results_df_optimized_all_metrics, pd.DataFrame([stacked_results])], ignore_index=True)\n",
        "\n",
        "#print(\"\\nOptimized Model Performance (Including Stacked Model):\")\n",
        "#print(results_df_optimized_all_metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b86d0295"
      },
      "source": [
        "# Step 9: Generate Stacked Predictions for Train and Validation\n",
        "\n",
        "# Generate base model predictions on the train set (using base models trained on the full train set)\n",
        "train_preds_base_models = np.zeros((X_train.shape[0], len(selected_base_models)))\n",
        "for i, (name, model) in enumerate(selected_base_models):\n",
        "    train_preds_base_models[:, i] = model.predict(X_train).flatten()\n",
        "\n",
        "# Generate stacked predictions on the train set using the Ridge meta-learner\n",
        "stacked_train_preds = meta_learner_ridge_optimized.predict(train_preds_base_models)\n",
        "\n",
        "\n",
        "# Generate base model predictions on the validation set (using base models trained on the full train set)\n",
        "val_preds_base_models = np.zeros((X_val.shape[0], len(selected_base_models)))\n",
        "for i, (name, model) in enumerate(selected_base_models):\n",
        "    val_preds_base_models[:, i] = model.predict(X_val).flatten()\n",
        "\n",
        "# Generate stacked predictions on the validation set using the Ridge meta-learner\n",
        "stacked_val_preds = meta_learner_ridge_optimized.predict(val_preds_base_models)\n",
        "\n",
        "print(\"Stacked predictions generated for train and validation sets.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dae5afb7"
      },
      "source": [
        "# Step 10: Prepare DataFrames for Stacked Predictions\n",
        "\n",
        "# Train set DataFrame\n",
        "stacked_train_df_export = pd.DataFrame({\n",
        "    'Date': train_dates,\n",
        "    'Actual_CO2': y_train['CO2'],\n",
        "    'Predicted_CO2': stacked_train_preds.flatten() # Flatten in case of multi-output but single target\n",
        "})\n",
        "\n",
        "# Validation set DataFrame\n",
        "stacked_val_df_export = pd.DataFrame({\n",
        "    'Date': val_dates,\n",
        "    'Actual_CO2': y_val['CO2'],\n",
        "    'Predicted_CO2': stacked_val_preds.flatten()\n",
        "})\n",
        "\n",
        "# Test set DataFrame\n",
        "stacked_test_df_export = pd.DataFrame({\n",
        "    'Date': test_dates,\n",
        "    'Actual_CO2': y_test['CO2'],\n",
        "    'Predicted_CO2': stacked_test_preds.flatten()\n",
        "})\n",
        "\n",
        "print(\"DataFrames for stacked predictions created.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1a6d7dc"
      },
      "source": [
        "# Step 11: Export Stacked Prediction DataFrames to CSV\n",
        "\n",
        "stacked_train_df_export.to_csv('CO2/stacked_model_ridge_train_predictions.csv', index=False)\n",
        "print(\"Stacked train predictions exported to CO2/stacked_model_ridge_train_predictions.csv\")\n",
        "\n",
        "stacked_val_df_export.to_csv('CO2/stacked_model_ridge_val_predictions.csv', index=False)\n",
        "print(\"Stacked validation predictions exported to CO2/stacked_model_ridge_val_predictions.csv\")\n",
        "\n",
        "stacked_test_df_export.to_csv('CO2/stacked_model_ridge_test_predictions.csv', index=False)\n",
        "print(\"Stacked test predictions exported to CO2/stacked_model_ridge_test_predictions.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fa36e3f"
      },
      "source": [
        "#step 12 Calculate and display evaluation metrics for the stacked model on Train, Validation, and Test sets\n",
        "\n",
        "# Train set metrics\n",
        "stacked_train_rmse = np.sqrt(mean_squared_error(y_train, stacked_train_preds))\n",
        "stacked_train_r2 = r2_score(y_train, stacked_train_preds)\n",
        "stacked_train_mae = mean_absolute_error(y_train, stacked_train_preds)\n",
        "stacked_train_mape = mean_absolute_percentage_error(y_train, stacked_train_preds)\n",
        "\n",
        "print(\"Stacked Model Train RMSE:\", stacked_train_rmse)\n",
        "print(\"Stacked Model Train R2:\", stacked_train_r2)\n",
        "print(\"Stacked Model Train MAE:\", stacked_train_mae)\n",
        "print(\"Stacked Model Train MAPE:\", stacked_train_mape)\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Validation set metrics\n",
        "stacked_val_rmse = np.sqrt(mean_squared_error(y_val, stacked_val_preds))\n",
        "stacked_val_r2 = r2_score(y_val, stacked_val_preds)\n",
        "stacked_val_mae = mean_absolute_error(y_val, stacked_val_preds)\n",
        "stacked_val_mape = mean_absolute_percentage_error(y_val, stacked_val_preds)\n",
        "\n",
        "print(\"Stacked Model Val RMSE:\", stacked_val_rmse)\n",
        "print(\"Stacked Model Val R2:\", stacked_val_r2)\n",
        "print(\"Stacked Model Val MAE:\", stacked_val_mae)\n",
        "print(\"Stacked Model Val MAPE:\", stacked_val_mape)\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Test set metrics (already calculated, but displaying again for completeness)\n",
        "print(\"Stacked Model Test RMSE:\", stacked_test_rmse)\n",
        "print(\"Stacked Model Test R2:\", stacked_test_r2)\n",
        "print(\"Stacked Model Test MAE:\", stacked_test_mae)\n",
        "print(\"Stacked Model Test MAPE:\", stacked_test_mape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SHAP EXPLANATION**"
      ],
      "metadata": {
        "id": "RmmqDqus_DOT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5881500c"
      },
      "source": [
        "%pip install shap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b05d7df1"
      },
      "source": [
        "# Step 2: Define Stacked Model Prediction Function\n",
        "\n",
        "def stacked_model_predict(X):\n",
        "    \"\"\"\n",
        "    Prediction function for the stacked model that takes original features (X)\n",
        "    and returns the final stacked predictions.\n",
        "    \"\"\"\n",
        "    # Ensure X is a pandas DataFrame to use .iloc for indexing in base models\n",
        "    if not isinstance(X, pd.DataFrame):\n",
        "        X = pd.DataFrame(X, columns=X_train.columns) # Assuming column names are consistent\n",
        "\n",
        "    # Generate predictions from base models\n",
        "    base_model_preds = np.zeros((X.shape[0], len(selected_base_models)))\n",
        "    for i, (name, model) in enumerate(selected_base_models):\n",
        "        base_model_preds[:, i] = model.predict(X).flatten()\n",
        "\n",
        "    # Generate final predictions using the meta-learner\n",
        "    final_predictions = meta_learner_ridge_optimized.predict(base_model_preds)\n",
        "\n",
        "    return final_predictions.flatten() # Return flattened predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e966e425"
      },
      "source": [
        "# Step 3: Prepare Data for SHAP and Initialize Explainer\n",
        "\n",
        "import shap\n",
        "\n",
        "# Select data subsets for explanation\n",
        "# Using the full datasets as requested. Be aware that Kernel SHAP can be computationally intensive.\n",
        "X_train_sample = X_train # Use full training data for SHAP explanation\n",
        "X_val_sample = X_val   # Use full validation data for SHAP explanation\n",
        "X_test_sample = X_test   # Use full test data for SHAP explanation\n",
        "\n",
        "\n",
        "# Use a background dataset for the explainer (e.g., a small sample of the training data)\n",
        "# A smaller background dataset is generally sufficient and helps reduce computation time.\n",
        "background_data = X_train.sample(min(200, len(X_train)), random_state=42)\n",
        "\n",
        "\n",
        "# Initialize the Kernel SHAP explainer\n",
        "# Passing the prediction function and the background dataset\n",
        "explainer = shap.KernelExplainer(stacked_model_predict, background_data)\n",
        "\n",
        "print(\"Data subsets prepared and SHAP Kernel Explainer initialized.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4fd657f"
      },
      "source": [
        "# Step 4: Compute SHAP Values\n",
        "\n",
        "print(\"Computing SHAP values for train sample...\")\n",
        "shap_values_train = explainer.shap_values(X_train_sample)\n",
        "\n",
        "print(\"Computing SHAP values for validation sample...\")\n",
        "shap_values_val = explainer.shap_values(X_val_sample)\n",
        "\n",
        "print(\"Computing SHAP values for test sample...\")\n",
        "shap_values_test = explainer.shap_values(X_test_sample)\n",
        "\n",
        "print(\"SHAP value computation completed.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de921711"
      },
      "source": [
        "# Step 5: Generate SHAP Summary Plot (for Test Sample)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Create the CO2 folder if it doesn't exist\n",
        "if not os.path.exists('CO2'):\n",
        "    os.makedirs('CO2')\n",
        "\n",
        "print(\"Generating SHAP summary plot (bar) for the test sample and exporting to JPEG...\")\n",
        "# Use plot_type=\"bar\" for overall feature importance\n",
        "shap.summary_plot(shap_values_test, X_test_sample, plot_type=\"bar\", show=False)\n",
        "plt.title(\"SHAP Feature Importance (Test Sample)\")\n",
        "plt.savefig('CO2/shap_feature_importance_bar.jpeg', dpi=350, bbox_inches='tight')\n",
        "plt.show() # Display the plot in the notebook\n",
        "plt.close() # Close the plot to prevent it from displaying twice\n",
        "\n",
        "print(\"Generating SHAP summary plot (default) for the test sample and exporting to JPEG...\")\n",
        "# Default summary plot showing impact and direction\n",
        "shap.summary_plot(shap_values_test, X_test_sample, show=False)\n",
        "plt.title(\"SHAP Summary Plot (Test Sample)\")\n",
        "plt.savefig('CO2/shap_summary_plot.jpeg', dpi=350, bbox_inches='tight')\n",
        "plt.show() # Display the plot in the notebook\n",
        "plt.close() # Close the plot\n",
        "\n",
        "print(\"SHAP summary plots generated and exported to CO2 folder.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependence plot"
      ],
      "metadata": {
        "id": "t7zlQVsQILKh"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebabe745"
      },
      "source": [
        "# Step 6: Generate SHAP Dependence Plots (for key features in Test Sample)\n",
        "\n",
        "# Identify the top features from the summary plot (e.g., based on the bar plot)\n",
        "# Let's assume the features are sorted by importance in the bar plot.\n",
        "# We can pick a few of the top features to visualize.\n",
        "top_features = X_test_sample.columns[np.argsort(np.abs(shap_values_test).mean(0))[::-1]]\n",
        "\n",
        "# Choose the top N features to plot dependence for\n",
        "num_dependence_plots = min(3, len(top_features)) # Plot for top 3 features or fewer if less than 3\n",
        "\n",
        "print(f\"Generating SHAP dependence plots for the top {num_dependence_plots} features and exporting to JPEG...\")\n",
        "\n",
        "for i in range(num_dependence_plots):\n",
        "    shap.dependence_plot(top_features[i], shap_values_test, X_test_sample, interaction_index=None, show=False)\n",
        "    plt.title(f\"SHAP Dependence Plot for {top_features[i]} (Test Sample)\")\n",
        "    plt.savefig(f'CO2/shap_dependence_plot_{top_features[i]}.jpeg', dpi=350, bbox_inches='tight')\n",
        "    plt.show() # Display the plot in the notebook\n",
        "    plt.close() # Close the plot\n",
        "\n",
        "print(\"SHAP dependence plots generated and exported to CO2 folder.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Force plot"
      ],
      "metadata": {
        "id": "NxCCpl-PIPNH"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ce366de"
      },
      "source": [
        "# Generate SHAP Force Plot for a single instance (e.g., the first instance in the test set)\n",
        "\n",
        "# Select the instance you want to explain (e.g., the first instance)\n",
        "instance_idx = 0\n",
        "instance_to_explain = X_test_sample.iloc[[instance_idx]]\n",
        "shap_values_instance = explainer.shap_values(instance_to_explain)\n",
        "\n",
        "print(f\"Generating SHAP force plot for instance at index {instance_idx} in the test sample and exporting to JPEG...\")\n",
        "\n",
        "# Visualize the force plot\n",
        "shap.initjs() # Initialize JavaScript visualization in Colab\n",
        "# The force plot is interactive and cannot be directly saved as a static image using plt.savefig in the same way.\n",
        "# We will generate the HTML output which can be saved manually or by using other libraries if needed for static export.\n",
        "# For now, the force plot will be displayed as an interactive visualization.\n",
        "force_plot_html = shap.force_plot(explainer.expected_value, shap_values_instance, instance_to_explain, show=False).html()\n",
        "\n",
        "# To save the force plot as an image programmatically, you would typically need\n",
        "# to render the HTML in a headless browser or use libraries designed for this,\n",
        "# which is more complex than direct saving.\n",
        "# As a workaround, you can display the plot and save it manually, or we can\n",
        "# generate the HTML and you can save that.\n",
        "# Let's display the HTML output.\n",
        "display(HTML(force_plot_html))\n",
        "\n",
        "print(\"SHAP force plot generated (interactive HTML).\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbbdc2f9"
      },
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "# Find the instance with the median predicted value in the test set\n",
        "\n",
        "# Calculate the median of the stacked test predictions\n",
        "median_prediction = np.median(stacked_test_preds)\n",
        "\n",
        "# Find the index of the instance in the test set with the prediction closest to the median\n",
        "# We find the absolute difference between each prediction and the median prediction,\n",
        "# and then get the index of the minimum difference.\n",
        "closest_instance_index_in_test_sample = np.argmin(np.abs(stacked_test_preds - median_prediction))\n",
        "\n",
        "# Get the actual instance data from the test sample using this index\n",
        "instance_to_explain_median = X_test_sample.iloc[[closest_instance_index_in_test_sample]]\n",
        "\n",
        "# Get the SHAP values for this specific instance from the pre-computed SHAP values for the test sample\n",
        "shap_values_median_instance = shap_values_test[closest_instance_index_in_test_sample]\n",
        "\n",
        "print(f\"Generating SHAP force plot for the instance with a prediction closest to the median predicted value ({median_prediction:.2f}) and exporting to JPEG...\")\n",
        "\n",
        "# Visualize the force plot for the median instance\n",
        "shap.initjs() # Initialize JavaScript visualization in Colab\n",
        "# The force plot is interactive and cannot be directly saved as a static image using plt.savefig in the same way.\n",
        "# We will generate the HTML output which can be saved manually or by using other libraries if needed for static export.\n",
        "# For now, the force plot will be displayed as an interactive visualization.\n",
        "force_plot_median_html = shap.force_plot(explainer.expected_value, shap_values_median_instance, instance_to_explain_median, show=False).html()\n",
        "\n",
        "# Let's display the HTML output.\n",
        "display(HTML(force_plot_median_html))\n",
        "\n",
        "print(\"SHAP force plot for median instance generated (interactive HTML).\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "d45fba3c"
      },
      "source": [
        "# Compare Stacked Model Performance with Base Models on Validation and Test Sets\n",
        "\n",
        "# Extract relevant columns for base models from the results_df_optimized_all_metrics\n",
        "base_model_comparison = results_df_optimized_all_metrics[['Model', 'CO2_Train_RMSE', 'CO2_Train_R2', 'CO2_Train_MAE', 'CO2_Train_MAPE',\n",
        "                                                           'CO2_Val_RMSE', 'CO2_Val_R2', 'CO2_Val_MAE', 'CO2_Val_MAPE',\n",
        "                                                           'CO2_Test_RMSE', 'CO2_Test_R2', 'CO2_Test_MAE', 'CO2_Test_MAPE']].copy()\n",
        "\n",
        "# Create a DataFrame for the stacked model's performance\n",
        "stacked_model_comparison = pd.DataFrame([{\n",
        "    'Model': 'Stacked Model (Ridge Meta-Learner)',\n",
        "    'CO2_Train_RMSE': stacked_train_rmse,\n",
        "    'CO2_Train_R2': stacked_train_r2,\n",
        "    'CO2_Train_MAE': stacked_train_mae,\n",
        "    'CO2_Train_MAPE': stacked_train_mape,\n",
        "    'CO2_Val_RMSE': stacked_val_rmse,\n",
        "    'CO2_Val_R2': stacked_val_r2,\n",
        "    'CO2_Val_MAE': stacked_val_mae,\n",
        "    'CO2_Val_MAPE': stacked_val_mape,\n",
        "    'CO2_Test_RMSE': stacked_test_rmse,\n",
        "    'CO2_Test_R2': stacked_test_r2,\n",
        "    'CO2_Test_MAE': stacked_test_mae,\n",
        "    'CO2_Test_MAPE': stacked_test_mape\n",
        "}])\n",
        "\n",
        "# Concatenate the base model and stacked model results\n",
        "comparison_df = pd.concat([base_model_comparison, stacked_model_comparison], ignore_index=True)\n",
        "\n",
        "# Sort by Test R2 for easier comparison\n",
        "comparison_df = comparison_df.sort_values(by='CO2_Test_R2', ascending=False)\n",
        "\n",
        "print(\"Performance Comparison: Stacked Model vs. Base Models (Train, Validation, and Test Sets)\")\n",
        "display(comparison_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Export the final comparison DataFrame to a CSV file\n",
        "comparison_df.to_csv('CO2/final_model_comparison.csv', index=False)\n",
        "\n",
        "print(\"Final model comparison metrics exported to CO2/final_model_comparison.csv\")"
      ],
      "metadata": {
        "id": "A_bUY_xhHDiM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}